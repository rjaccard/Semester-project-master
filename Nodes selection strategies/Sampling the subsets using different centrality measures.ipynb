{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53eb479",
   "metadata": {},
   "source": [
    "The goal of this notebook is to find strategies that are able to find resolving sets as small as possible. We start with the Barabàsi-Albert model and then simulate the strategies on real graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from helpers import *\n",
    "import plotly.graph_objs as go\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804eeb49",
   "metadata": {},
   "source": [
    "## Barabási–Albert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, we will use the following parameters\n",
    "# n = 500\n",
    "# m = 30\n",
    "# nb_graph = 1\n",
    "# nb_of_iters = 100\n",
    "\n",
    "# can be used to speed up simulations as we know the theoritical value\n",
    "# lower_bound = 60\n",
    "# upper_bound = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_centrality(centrality_f, lower_bound=20, upper_bound=110, n=500, m=30, nb_graph=1, nb_of_iters=100, seed=0):\n",
    "    solutions_prob_c = {}\n",
    "    random.seed(seed)\n",
    "    for _ in range(nb_graph):\n",
    "        # Generate the random graph and compute shortest paths\n",
    "        G = nx.barabasi_albert_graph(n, m)\n",
    "        length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "        c = centrality_f(G)\n",
    "\n",
    "        # normalize the betweenness centralities\n",
    "        c_values = np.array(list(c.values()))\n",
    "        c_norm = c_values / sum(c_values)\n",
    "        node_list = list(c.keys())\n",
    "\n",
    "        for nb in range(0, lower_bound):\n",
    "            solutions_prob_c[nb] = solutions_prob_c.get(nb, 0) + 0  \n",
    "        for nb in tqdm(range(lower_bound, upper_bound)): # G.number_of_nodes()\n",
    "            num_nodes = nb # Number of nodes to sample\n",
    "            count = 0\n",
    "            for i in range(nb_of_iters):\n",
    "                nodes = set(np.random.choice(node_list, p=c_norm, size=num_nodes, replace=False)) # Random set of nodes to test\n",
    "                if is_resolving_set(G, nodes, length):\n",
    "                    count += 1\n",
    "            solutions_prob_c[nb] =  solutions_prob_c.get(nb, 0) + (count / nb_of_iters)\n",
    "        for nb in range(upper_bound, n):\n",
    "                solutions_prob_c[nb] = solutions_prob_c.get(nb, 0) + 1\n",
    "\n",
    "    for i in range(n):\n",
    "        solutions_prob_c[i] = solutions_prob_c[i] / nb_graph\n",
    "        \n",
    "    return solutions_prob_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_prob_bc_2 = sim_centrality(nx.betweenness_centrality, lower_bound=20, upper_bound=80, nb_graph=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a1959",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_prob_eigen = sim_centrality(nx.eigenvector_centrality, lower_bound=30, upper_bound=90, nb_graph=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_prob_dc = sim_centrality(nx.degree_centrality, lower_bound=30, upper_bound=90, nb_graph=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e28c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_prob_cc = sim_centrality(nx.closeness_centrality, lower_bound=20, upper_bound=100, nb_graph=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_prob_flow_bc = sim_centrality(nx.current_flow_betweenness_centrality, lower_bound=30, upper_bound=90, nb_graph=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb18a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_prob_so = sim_centrality(nx.second_order_centrality, lower_bound=25, upper_bound=90, nb_graph=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b9852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random strategy\n",
    "\n",
    "n = 500\n",
    "m = 30\n",
    "nb_graph = 10\n",
    "s = {}\n",
    "nb_of_iters = 100\n",
    "\n",
    "# can be used to speed up simulations as we know the theoritical value\n",
    "lower_bound = 20\n",
    "upper_bound = 100\n",
    "\n",
    "solutions_rand = {}\n",
    "random.seed(0)\n",
    "for _ in range(nb_graph):\n",
    "    # Generate the random graph and compute shortest paths\n",
    "    G = nx.barabasi_albert_graph(n, m)\n",
    "    length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    \n",
    "    for nb in range(0, lower_bound):\n",
    "        solutions_rand[nb] = solutions_rand.get(nb, 0) + 0  \n",
    "    for nb in tqdm(range(lower_bound, upper_bound)): # G.number_of_nodes()\n",
    "        num_nodes = nb # Number of nodes to sample\n",
    "        node_list = list(G.nodes())\n",
    "        count = 0\n",
    "        for i in range(nb_of_iters):\n",
    "            nodes = set(random.sample(node_list, num_nodes)) # Random set of nodes to test\n",
    "            if is_resolving_set(G, nodes, length):\n",
    "                count += 1\n",
    "        solutions_rand[nb] =  solutions_rand.get(nb, 0) + (count / nb_of_iters)\n",
    "    for nb in range(upper_bound, n):\n",
    "            solutions_rand[nb] = solutions_rand.get(nb, 0) + 1\n",
    "\n",
    "for i in range(n):\n",
    "    solutions_rand[i] = solutions_rand[i] / nb_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c5301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Strategy where we draw only from first quntile (degree)\n",
    "\n",
    "n = 500\n",
    "m = 30\n",
    "nb_of_iters = 100\n",
    "nb_graphs = 10\n",
    "\n",
    "solutions_first_q = {}\n",
    "\n",
    "lower_bound = 20\n",
    "upper_bound = 90\n",
    "\n",
    "number_of_quintile = 5\n",
    "\n",
    "for _ in range(nb_graphs):\n",
    "    G = nx.barabasi_albert_graph(n, m)\n",
    "\n",
    "    node_list = list(G.nodes())\n",
    "\n",
    "    degree_list = [(n, d) for n, d in G.degree()]\n",
    "    degree_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    num_vertices = len(degree_list)\n",
    "    num_vertices_per_decile = n // number_of_quintile\n",
    "    decile_num = 1\n",
    "    decile_vertices = []\n",
    "\n",
    "    for i in range(n):\n",
    "        vertex = degree_list[i][0]\n",
    "        degree = degree_list[i][1]\n",
    "        decile_vertices.append(vertex)\n",
    "        if (i + 1) % num_vertices_per_decile == 0:\n",
    "            nx.set_node_attributes(G, {v: decile_num for v in decile_vertices}, 'decile')\n",
    "            decile_num += 1\n",
    "            decile_vertices = []\n",
    "\n",
    "    length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "    quintile = 1      \n",
    "    nodes_in_decile = [node for node, decile in nx.get_node_attributes(G, 'decile').items() if decile == quintile]\n",
    "    \n",
    "    \n",
    "    for nb in range(0, lower_bound):\n",
    "        solutions_first_q[nb] = solutions_first_q.get(nb, 0) + 0  \n",
    "    for nb in tqdm(range(lower_bound, upper_bound)): # G.number_of_nodes()\n",
    "        num_nodes = nb # Number of nodes to sample\n",
    "        node_list = list(G.nodes())\n",
    "        count = 0\n",
    "        for i in range(nb_of_iters):\n",
    "            nodes = set(random.sample(nodes_in_decile, num_nodes)) # Random set of nodes to test\n",
    "            if is_resolving_set(G, nodes, length):\n",
    "                count += 1\n",
    "        solutions_first_q[nb] =  solutions_first_q.get(nb, 0) + (count / nb_of_iters)\n",
    "    for nb in range(upper_bound, n):\n",
    "            solutions_first_q[nb] = solutions_first_q.get(nb, 0) + 1\n",
    "    \n",
    "for i in range(n):\n",
    "    solutions_first_q[i] = solutions_first_q[i] / nb_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ecf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    solutions_first_q[i] = solutions_first_q[i] / nb_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3af03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('simulations/transition_eigen_centrality.pickle', 'wb') as file:\n",
    "    #pickle.dump(solutions_prob_eigen, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can be used to directyl import the results of the simulations\n",
    "\n",
    "\n",
    "#with open('simulations/Barbasi-Albert/transition_random.pickle', \"rb\") as file:\n",
    "    #solutions_rand = pickle.load(file)\n",
    "#with open('simulations/Barbasi-Albert/transition_betweenness_centrality.pickle', \"rb\") as file:\n",
    "    #solutions_prob_bc = pickle.load(file)\n",
    "#with open('simulations/Barbasi-Albert/transition_degree_centrality.pickle', \"rb\") as file:\n",
    "    #solutions_prob_dc = pickle.load(file)\n",
    "#with open('simulations/Barbasi-Albert/transition_so_centrality.pickle', \"rb\") as file:\n",
    "    #solutions_prob_so = pickle.load(file)\n",
    "#with open('simulations/Barbasi-Albert/transition_closness_centrality.pickle', \"rb\") as file:\n",
    "    #solutions_prob_cc = pickle.load(file)\n",
    "#with open('simulations/Barbasi-Albert/transition_eigen_centrality.pickle', \"rb\") as file:\n",
    "    #solutions_prob_eigen = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dddab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data\n",
    "x = list(solutions_rand.keys())\n",
    "y = list(solutions_rand.values())\n",
    "\n",
    "x1 = list(solutions_first_q.keys())\n",
    "y1 = list(solutions_first_q.values())\n",
    "\n",
    "x3 = list(solutions_prob_bc.keys())\n",
    "y3 = list(solutions_prob_bc.values())\n",
    "\n",
    "x4 = list(solutions_prob_dc.keys())\n",
    "y4 = list(solutions_prob_dc.values())\n",
    "\n",
    "x5 = list(solutions_prob_so.keys())\n",
    "y5 = list(solutions_prob_so.values())\n",
    "\n",
    "x6 = list(solutions_prob_cc.keys())\n",
    "y6 = list(solutions_prob_cc.values())\n",
    "\n",
    "x8 = list(solutions_prob_eigen.keys())\n",
    "y8 = list(solutions_prob_eigen.values())\n",
    "\n",
    "# Define the trace for the scatter plot\n",
    "trace = go.Scatter(x=x, y=y, mode='markers+lines', name='Random nodes')\n",
    "trace1 = go.Scatter(x=x1, y=y1, mode='markers+lines', name='Nodes with high degree (top 20%)')\n",
    "trace3 = go.Scatter(x=x3, y=y3, mode='markers+lines', name='Betweenness centrality')\n",
    "trace4 = go.Scatter(x=x4, y=y4, mode='markers+lines', name='Degree centrality')\n",
    "trace5 = go.Scatter(x=x5, y=y5, mode='markers+lines', name='Second order centrality')\n",
    "trace6 = go.Scatter(x=x6, y=y6, mode='markers+lines', name='Closeness centrality')\n",
    "trace8 = go.Scatter(x=x8, y=y8, mode='markers+lines', name='Eigenvector centrality')\n",
    "\n",
    "\n",
    "# Define the layout\n",
    "layout = go.Layout(#title='Probability of resolving the graph as a function of the subset cardinality', \n",
    "                   #title_x=0.5,\n",
    "                   xaxis=dict(title='Cardinality of the subset'), \n",
    "                   yaxis=dict(title='Probability of resolving the graph'),\n",
    "                   legend=dict(x=0.67, y=0.08, orientation='v'))\n",
    "\n",
    "# Combine the traces and layout into a figure\n",
    "fig = go.Figure(data=[trace, trace1, trace3, trace4, trace5, trace6, trace8], layout=layout)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_centrality_box(centrality_f, n=500, m=30, nb_graph=15, nb_of_iters=100, seed=0):\n",
    "    \n",
    "    box_sol = []\n",
    "    random.seed(seed)\n",
    "    for _ in tqdm(range(nb_graph)):\n",
    "    \n",
    "        # Generate the random graph and compute shortest paths\n",
    "        G = nx.barabasi_albert_graph(n, m)\n",
    "        length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "        c = centrality_f(G)\n",
    "\n",
    "        # normalize the betweenness centralities\n",
    "        c_values = np.array(list(c.values()))\n",
    "        c_norm = c_values / sum(c_values)\n",
    "        node_list = list(c.keys())\n",
    "        for _ in range(nb_of_iters):\n",
    "            nodes = np.random.choice(node_list, p=c_norm, size=len(G), replace=False) # Random set of nodes to test\n",
    "            for nb in range(0, len(G) + 1):\n",
    "                #print(nb)\n",
    "                if is_resolving_set(G, nodes[:nb], length):\n",
    "                    box_sol.append(nb)\n",
    "                    break\n",
    "    return box_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7119c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random\n",
    "\n",
    "n = 500\n",
    "m = 30\n",
    "nb_graph = 15\n",
    "nb_of_iters = 100\n",
    "\n",
    "rand_box_sol = []\n",
    "random.seed(0)\n",
    "for _ in range(nb_graph):\n",
    "\n",
    "    # Generate the random graph and compute shortest paths\n",
    "    G = nx.barabasi_albert_graph(n, m)\n",
    "    length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "    for _ in tqdm(range(nb_of_iters)):\n",
    "        nodes = list(random.sample(list(G.nodes), n))\n",
    "        for nb in range(0, len(G)):\n",
    "            if is_resolving_set(G, nodes[:nb], length):\n",
    "                rand_box_sol.append(nb)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b85375",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_box_sol = sim_centrality_box(nx.degree_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_box_sol = sim_centrality_box(nx.betweenness_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b314a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_box_sol = sim_centrality_box(nx.closeness_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2aa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_box_sol = sim_centrality_box(nx.second_order_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d574766",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_box_sol = sim_centrality_box(nx.eigenvector_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44014cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_result = {'Betweenness centrality': bc_box_sol, 'Degree centrality': dc_box_sol, 'Closeness centrality': cc_box_sol , 'Random': rand_box_sol, 'Second order centrality': so_box_sol, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a3807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('simulations/Barbasi-Albert/centrality_boxes_15graph_100iters.pickle', \"rb\") as file:\n",
    "    #dict_result = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df534608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('simulations/centrality_boxes_15graph_100iters.pickle', 'wb') as file:\n",
    "    #pickle.dump(dict_result, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b272bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for group, values in dict_result2.items():\n",
    "    fig.add_trace(go.Box(y=values, name=group))\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis=dict(\n",
    "        title=\"Size of the resolving set\",\n",
    "        titlefont=dict(size=12, color='black')\n",
    "    )\n",
    ")       \n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f236b",
   "metadata": {},
   "source": [
    "## Copenhagen fb_friends graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330622d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file and create a graph\n",
    "G_fb = nx.read_edgelist('../Real graphs simulations/Copenhagen graphs/fb_friends.csv/edges.csv', delimiter=',')\n",
    "\n",
    "# print the number of nodes and edges\n",
    "print('Number of nodes:', G_fb.number_of_nodes())\n",
    "print('Number of edges:', G_fb.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca01db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_centrality_real_g(centrality_f, lower_bound=20, upper_bound=800, nb_of_iters=400, seed=0, step=10):\n",
    "    solutions_prob_c = {}\n",
    "    random.seed(seed)\n",
    "    for _ in range(nb_graph):\n",
    "        # Generate the random graph and compute shortest paths\n",
    "        G = G_fb\n",
    "        length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "        c = centrality_f(G)\n",
    "\n",
    "        # normalize the betweenness centralities\n",
    "        c_values = np.array(list(c.values()))\n",
    "        c_norm = c_values / sum(c_values)\n",
    "        node_list = list(c.keys())\n",
    "\n",
    "        for nb in range(0, lower_bound):\n",
    "            solutions_prob_c[nb] = solutions_prob_c.get(nb, 0) + 0  \n",
    "        for nb in tqdm(range(lower_bound, upper_bound+1, step)): # G.number_of_nodes()\n",
    "            num_nodes = nb # Number of nodes to sample\n",
    "            count = 0\n",
    "            for i in range(nb_of_iters):\n",
    "                nodes = set(np.random.choice(node_list, p=c_norm, size=num_nodes, replace=False)) # Random set of nodes to test\n",
    "                if is_resolving_set(G, nodes, length):\n",
    "                    count += 1\n",
    "            solutions_prob_c[nb] =  solutions_prob_c.get(nb, 0) + (count / nb_of_iters)\n",
    "        for nb in range(upper_bound+1, G_fb.number_of_nodes()+1):\n",
    "                solutions_prob_c[nb] = solutions_prob_c.get(nb, 0) + 1\n",
    "        \n",
    "    return solutions_prob_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c60b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_prob_eigen_fb = sim_centrality_real_g(nx.eigenvector_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8518726",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_prob_dc_fb = sim_centrality_real_g(nx.degree_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d486da",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_prob_cc_fb = sim_centrality_real_g(nx.closeness_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192dc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_prob_so_fb = sim_centrality_real_g(nx.second_order_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random strategy\n",
    "\n",
    "nb_graph = 1\n",
    "s = {}\n",
    "nb_of_iters = 800\n",
    "step = 10\n",
    "\n",
    "# can be used to speed up simulations\n",
    "lower_bound = 20\n",
    "upper_bound = 800\n",
    "\n",
    "solutions_rand_fb = {}\n",
    "random.seed(0)\n",
    "for _ in range(nb_graph):\n",
    "    # Generate the random graph and compute shortest paths\n",
    "    G = G_fb\n",
    "    length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    \n",
    "    for nb in range(0, lower_bound):\n",
    "        solutions_rand_fb[nb] = solutions_rand_fb.get(nb, 0) + 0  \n",
    "    for nb in tqdm(range(lower_bound, upper_bound+1, step)):\n",
    "        num_nodes = nb # Number of nodes to sample\n",
    "        node_list = list(G.nodes())\n",
    "        count = 0\n",
    "        for i in range(nb_of_iters):\n",
    "            nodes = set(random.sample(node_list, num_nodes)) # Random set of nodes to test\n",
    "            if is_resolving_set(G, nodes, length):\n",
    "                count += 1\n",
    "        solutions_rand_fb[nb] =  solutions_rand_fb.get(nb, 0) + (count / nb_of_iters)\n",
    "    for nb in range(upper_bound+1, G_fb.number_of_nodes()+1):\n",
    "            solutions_rand_fb[nb] = solutions_rand_fb.get(nb, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c624e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betweenness centrality\n",
    "\n",
    "# can be used to speed up simulations\n",
    "lower_bound = 20\n",
    "upper_bound = 800\n",
    "nb_of_iters = 800\n",
    "step = 10\n",
    "solutions_prob_bc_fb = {}\n",
    "for _ in range(nb_graph):\n",
    "    # Generate the random graph and compute shortest paths\n",
    "    G = G_fb\n",
    "    length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "    c = nx.betweenness_centrality(G, endpoints=True)\n",
    "\n",
    "    # normalize the betweenness centralities\n",
    "    c_values = np.array(list(c.values()))\n",
    "    c_norm = c_values / sum(c_values)\n",
    "    node_list = list(c.keys())\n",
    "\n",
    "    for nb in range(0, lower_bound):\n",
    "        solutions_prob_bc_fb[nb] = solutions_prob_bc_fb.get(nb, 0) + 0  \n",
    "    for nb in tqdm(range(lower_bound, upper_bound+1, step)): # G.number_of_nodes()\n",
    "        num_nodes = nb # Number of nodes to sample\n",
    "        count = 0\n",
    "        for i in range(nb_of_iters):\n",
    "            nodes = set(np.random.choice(node_list, p=c_norm, size=num_nodes, replace=False)) # Random set of nodes to test\n",
    "            if is_resolving_set(G, nodes, length):\n",
    "                count += 1\n",
    "        solutions_prob_bc_fb[nb] = solutions_prob_bc_fb.get(nb, 0) + (count / nb_of_iters)\n",
    "    for nb in range(upper_bound+1, G_fb.number_of_nodes()+1):\n",
    "            solutions_prob_bc_fb[nb] = solutions_prob_bc_fb.get(nb, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a527bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boost the proba of low degree nodes\n",
    "\n",
    "lower_bound = 20\n",
    "upper_bound = 40\n",
    "\n",
    "solutions_prob_ms_d = {}\n",
    "for _ in range(nb_graph):\n",
    "    # Generate the random graph and compute shortest paths\n",
    "    G = G_fb\n",
    "    length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "    c = nx.degree_centrality(G)\n",
    "    a = list(set(c.values()))\n",
    "    a.sort()\n",
    "    c = {key: 1000*max(c.values()) if value in a[:2] else value for key, value in c.items()}\n",
    "    # normalize the betweenness centralities\n",
    "    c_values = np.array(list(c.values()))\n",
    "    c_norm = c_values / sum(c_values)\n",
    "    node_list = list(c.keys())\n",
    "\n",
    "    for nb in range(0, lower_bound):\n",
    "        solutions_prob_ms_d[nb] = solutions_prob_ms_d.get(nb, 0) + 0  \n",
    "    for nb in tqdm(range(lower_bound, upper_bound, 1)): # G.number_of_nodes()\n",
    "        num_nodes = nb # Number of nodes to sample\n",
    "        count = 0\n",
    "        for i in range(nb_of_iters):\n",
    "            nodes = set(np.random.choice(node_list, p=c_norm, size=num_nodes, replace=False)) # Random set of nodes to test\n",
    "            if is_resolving_set(G, nodes, length):\n",
    "                count += 1\n",
    "        solutions_prob_ms_d[nb] =  solutions_prob_ms_d.get(nb, 0) + (count / nb_of_iters)\n",
    "    for nb in range(upper_bound, G_fb.number_of_nodes()):\n",
    "            solutions_prob_ms_d[nb] = solutions_prob_ms_d.get(nb, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boost the proba of low betweenness nodes\n",
    "\n",
    "lower_bound = 20\n",
    "upper_bound = 40\n",
    "\n",
    "solutions_prob_ms_bc = {}\n",
    "for _ in range(nb_graph):\n",
    "    # Generate the random graph and compute shortest paths\n",
    "    G = G_fb\n",
    "    length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "    c = nx.betweenness_centrality(G)\n",
    "    a = list(set(c.values()))\n",
    "    a.sort()\n",
    "    #c = {key: -(v-max(c.values())-min(c.values())) for key, v in c.items()}\n",
    "    c = {key: 1000*max(c.values()) if value in a[:1] else value for key, value in c.items()}\n",
    "    # normalize the betweenness centralities\n",
    "    c_values = np.array(list(c.values()))\n",
    "    c_norm = c_values / sum(c_values)\n",
    "    node_list = list(c.keys())\n",
    "\n",
    "    for nb in range(0, lower_bound):\n",
    "        solutions_prob_ms_bc[nb] = solutions_prob_ms_bc.get(nb, 0) + 0  \n",
    "    for nb in tqdm(range(lower_bound, upper_bound, 1)): # G.number_of_nodes()\n",
    "        num_nodes = nb # Number of nodes to sample\n",
    "        count = 0\n",
    "        for i in range(nb_of_iters):\n",
    "            nodes = set(np.random.choice(node_list, p=c_norm, size=num_nodes, replace=False)) # Random set of nodes to test\n",
    "            if is_resolving_set(G, nodes, length):\n",
    "                count += 1\n",
    "        solutions_prob_ms_bc[nb] =  solutions_prob_ms_bc.get(nb, 0) + (count / nb_of_iters)\n",
    "    for nb in range(upper_bound, G_fb.number_of_nodes()):\n",
    "            solutions_prob_ms_bc[nb] = solutions_prob_ms_bc.get(nb, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1 with degree\n",
    "\n",
    "lower_bound = 20\n",
    "upper_bound = 40\n",
    "\n",
    "solutions_prob_ms_bc_add = {}\n",
    "for _ in range(nb_graph):\n",
    "    # Generate the random graph and compute shortest paths\n",
    "    G = G_fb\n",
    "\n",
    "    nb_of_nodes_in_resolving_set = 20\n",
    "\n",
    "    length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "    c = nx.degree_centrality(G)\n",
    "\n",
    "    intensities = {str(key): 0 for key in G.nodes}\n",
    "\n",
    "    # normalize the betweenness centralities\n",
    "    c_values = np.array(list(c.values()))\n",
    "    c_norm = c_values / sum(c_values)\n",
    "    node_list = list(c.keys())\n",
    "    \n",
    "    for i in range(100):\n",
    "        # Random set of nodes to test\n",
    "        nodes = set(np.random.choice(node_list, p=c_norm, size=nb_of_nodes_in_resolving_set, replace=False))\n",
    "        resolved = set_resolved(G, nodes, length)\n",
    "        for node in resolved:\n",
    "            intensities[node] += +1\n",
    "    \n",
    "    small = [key for key, val in intensities.items() if val < 40]\n",
    "    nodes_to_add = get_nodes_with_diff_neighbors(G, small)\n",
    "    \n",
    "    for nb in range(0, lower_bound):\n",
    "        solutions_prob_ms_bc_add[nb] = solutions_prob_ms_bc_add.get(nb, 0) + 0  \n",
    "    for nb in tqdm(range(lower_bound, upper_bound, 1)): # G.number_of_nodes()\n",
    "        num_nodes = nb - len(nodes_to_add) # Number of nodes to sample\n",
    "        count = 0\n",
    "        for i in range(nb_of_iters):\n",
    "            nodes = nodes_to_add + list(np.random.choice(node_list, p=c_norm, size=num_nodes, replace=False)) # Random set of nodes to test\n",
    "            if is_resolving_set(G, nodes, length):\n",
    "                count += 1\n",
    "        solutions_prob_ms_bc_add[nb] =  solutions_prob_ms_bc_add.get(nb, 0) + (count / nb_of_iters)\n",
    "    for nb in range(upper_bound, G_fb.number_of_nodes()):\n",
    "            solutions_prob_ms_bc_add[nb] = solutions_prob_ms_bc_add.get(nb, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81f2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1 with betweenness\n",
    "\n",
    "lower_bound = 20\n",
    "upper_bound = 40\n",
    "nb_of_iters = 4\n",
    "solutions_prob_ms_bc_add = {}\n",
    "\n",
    "G = G_fb\n",
    "nodes_to_add = []\n",
    "nb_of_nodes_in_resolving_set = 20\n",
    "\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "c = nx.betweenness_centrality(G)\n",
    "\n",
    "# normalize the betweenness centralities\n",
    "c_values = np.array(list(c.values()))\n",
    "c_norm = c_values / sum(c_values)\n",
    "node_list = list(c.keys())\n",
    "\n",
    "while True:\n",
    "    intensities = {str(key): 0 for key in G.nodes}\n",
    "    for i in range(100):\n",
    "        # Random set of nodes to test\n",
    "        nodes = set(np.random.choice(node_list, p=c_norm, size=nb_of_nodes_in_resolving_set, replace=False))\n",
    "        resolved = set_resolved(G, nodes, length)\n",
    "        for node in resolved:\n",
    "            intensities[node] += +1\n",
    "\n",
    "    small = [key for key, val in intensities.items() if val < 40]\n",
    "    s = len(small)\n",
    "    nodes_to_add += get_nodes_with_diff_neighbors(G, small)\n",
    "    if s == 0:\n",
    "        break\n",
    "indices = [node_list.index(i) for i in nodes_to_add] \n",
    "node_list = [element for index, element in enumerate(node_list) if index not in indices]\n",
    "c_norm = [element for index, element in enumerate(c_norm) if index not in indices]\n",
    "c_norm = c_norm / sum(c_norm)\n",
    "\n",
    "for nb in range(0, len(nodes_to_add)):\n",
    "    solutions_prob_ms_bc_add[nb] = solutions_prob_ms_bc_add.get(nb, 0) + 0  \n",
    "for nb in tqdm(range(len(nodes_to_add), upper_bound, 1)): # G.number_of_nodes()\n",
    "    num_nodes = nb - len(nodes_to_add) # Number of nodes to sample\n",
    "    count = 0\n",
    "    for i in range(nb_of_iters):\n",
    "        nodes = nodes_to_add + list(np.random.choice(node_list, p=c_norm, size=num_nodes, replace=False)) # Random set of nodes to test\n",
    "        if is_resolving_set(G, nodes, length):\n",
    "            count += 1\n",
    "    solutions_prob_ms_bc_add[nb] =  solutions_prob_ms_bc_add.get(nb, 0) + (count / nb_of_iters)\n",
    "for nb in range(upper_bound, G_fb.number_of_nodes()):\n",
    "        solutions_prob_ms_bc_add[nb] = solutions_prob_ms_bc_add.get(nb, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('simulations/fb_transition_eigen_400iters_10step.pickle', 'wb') as file:\n",
    "    #pickle.dump(solutions_prob_eigen_fb, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ced58d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define your data\n",
    "x = list(solutions_rand_fb.keys())\n",
    "y = list(solutions_rand_fb.values())\n",
    "\n",
    "\n",
    "x2 = list(solutions_prob_d.keys())\n",
    "y2 = list(solutions_prob_d.values())\n",
    "\n",
    "x3 = list(solutions_prob_bc_fb.keys())\n",
    "y3 = list(solutions_prob_bc_fb.values())\n",
    "\n",
    "x4 = list(solutions_prob_dc_fb.keys())\n",
    "y4 = list(solutions_prob_dc_fb.values())\n",
    "\n",
    "x5 = list(solutions_prob_so_fb.keys())\n",
    "y5 = list(solutions_prob_so_fb.values())\n",
    "\n",
    "x6 = list(solutions_prob_cc_fb.keys())\n",
    "y6 = list(solutions_prob_cc_fb.values())\n",
    "\n",
    "x7 = list(solutions_prob_ms_bc_add.keys())\n",
    "y7 = list(solutions_prob_ms_bc_add.values())\n",
    "\n",
    "x8 = list(solutions_prob_eigen_fb.keys())\n",
    "y8 = list(solutions_prob_eigen_fb.values())\n",
    "\n",
    "x9 = list(solutions_prob_ms_d.keys())\n",
    "y9 = list(solutions_prob_ms_d.values())\n",
    "\n",
    "x10 = list(solutions_prob_ms_bc.keys())\n",
    "y10 = list(solutions_prob_ms_bc.values())\n",
    "\n",
    "# Define the trace for the scatter plot\n",
    "trace = go.Scatter(x=x, y=y, mode='markers+lines', name='Random vertices')\n",
    "trace2 = go.Scatter(x=x2, y=y2, mode='markers+lines', name='Random vertices (prop to degree)')\n",
    "trace3 = go.Scatter(x=x3, y=y3, mode='markers+lines', name='Betweeness centrality')\n",
    "trace4 = go.Scatter(x=x4, y=y4, mode='markers+lines', name='Degree centrality')\n",
    "trace5 = go.Scatter(x=x5, y=y5, mode='markers+lines', name='Second order centrality)')\n",
    "trace6 = go.Scatter(x=x6, y=y6, mode='markers+lines', name='Closness centrality')\n",
    "trace7 = go.Scatter(x=x7, y=y7, mode='markers+lines', name='Magic sauce (betweenness + trick)')\n",
    "trace8 = go.Scatter(x=x8, y=y8, mode='markers+lines', name='Eigenvector centrality')\n",
    "trace9 = go.Scatter(x=x9, y=y9, mode='markers+lines', name='Magic Sauce (degree)')\n",
    "trace10 = go.Scatter(x=x10, y=y10, mode='markers+lines', name='Magic sauce (betweenness)')\n",
    "\n",
    "# Define the layout\n",
    "layout = go.Layout(#title='Probability of resolving the graph as a function of the subset cardinality', \n",
    "                   #title_x=0.5,\n",
    "                   xaxis=dict(title='Cardinality of the subset'), \n",
    "                   yaxis=dict(title='Probability of resolving the graph'))\n",
    "                   #legend=dict(x=0.67, y=0.08, orientation='v'))\n",
    "\n",
    "# Combine the traces and layout into a figure\n",
    "fig = go.Figure(data=[trace, trace1, trace3, trace4, trace5, trace6, trace7, trace8, trace9, trace10], layout=layout)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_centrality_box_real(G, centrality_f, nb_graph=1, nb_of_iters=400):\n",
    "    \n",
    "    box_sol = []\n",
    "    \n",
    "    # Generate the random graph and compute shortest paths\n",
    "    G = G_fb\n",
    "    length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "    c = centrality_f(G)\n",
    "\n",
    "    # normalize the centralities\n",
    "    c_values = np.array(list(c.values()))\n",
    "    c_norm = c_values / sum(c_values)\n",
    "    node_list = list(c.keys())\n",
    "\n",
    "    for _ in range(nb_of_iters):\n",
    "        nodes = np.random.choice(node_list, p=c_norm, size=len(G), replace=False) # Random set of nodes to test\n",
    "        for nb in range(0, len(G)):\n",
    "            if is_resolving_set(G, nodes[:nb], length):\n",
    "                box_sol.append(nb)\n",
    "                break\n",
    "                    \n",
    "    return box_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betweenness\n",
    "\n",
    "betweenness_box_sol = []\n",
    "\n",
    "nb_of_iters = 100\n",
    "\n",
    "# Generate the random graph and compute shortest paths\n",
    "G = G_fb\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "c = nx.betweenness_centrality(G, endpoints=True)\n",
    "\n",
    "# normalize the centralities\n",
    "c_values = np.array(list(c.values()))\n",
    "c_norm = c_values / sum(c_values)\n",
    "node_list = list(c.keys())\n",
    "\n",
    "for _ in tqdm(range(nb_of_iters)):\n",
    "    nodes = np.random.choice(node_list, p=c_norm, size=len(G), replace=False) # Random set of nodes to test\n",
    "    for nb in range(0, len(G)):\n",
    "        if is_resolving_set(G, nodes[:nb], length):\n",
    "            betweenness_box_sol.append(nb)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree\n",
    "\n",
    "degree_box_sol = []\n",
    "\n",
    "nb_of_iters = 100\n",
    "\n",
    "# Generate the random graph and compute shortest paths\n",
    "G = G_fb\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "c = nx.degree_centrality(G)\n",
    "\n",
    "# normalize the centralities\n",
    "c_values = np.array(list(c.values()))\n",
    "c_norm = c_values / sum(c_values)\n",
    "node_list = list(c.keys())\n",
    "\n",
    "for _ in range(nb_of_iters):\n",
    "    nodes = np.random.choice(node_list, p=c_norm, size=len(G), replace=False) # Random set of nodes to test\n",
    "    for nb in range(0, len(G)):\n",
    "        if is_resolving_set(G, nodes[:nb], length):\n",
    "            degree_box_sol.append(nb)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random\n",
    "\n",
    "nb_of_iters = 100\n",
    "rand_box_real = []\n",
    "\n",
    "# Generate the random graph and compute shortest paths\n",
    "G = G_fb\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "for _ in tqdm(range(nb_of_iters)):\n",
    "    nodes = list(random.sample(list(G.nodes), len(G)))\n",
    "    for nb in range(0, len(G)):\n",
    "        if is_resolving_set(G, nodes[:nb], length):\n",
    "            rand_box_real.append(nb)\n",
    "            print(nb)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45353c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1 with degree\n",
    "\n",
    "start1_degree_box_real = []\n",
    "\n",
    "tresh = 50\n",
    "nb_of_iters = 100\n",
    "\n",
    "G = G_fb\n",
    "nb_of_nodes_in_resolving_set = 20\n",
    "\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "c = nx.degree_centrality(G)\n",
    "\n",
    "# normalize the betweenness centralities\n",
    "c_values = np.array(list(c.values()))\n",
    "c_norm = c_values / sum(c_values)\n",
    "node_list = list(c.keys())\n",
    "ms_b_box_sol_real = []\n",
    "nodes_to_add = []\n",
    "\n",
    "while True:\n",
    "    intensities = {str(key): 0 for key in G.nodes}\n",
    "    for i in range(100):\n",
    "        # Random set of nodes to test\n",
    "        nodes = nodes_to_add + list(np.random.choice(node_list, p=c_norm, size=nb_of_nodes_in_resolving_set, replace=False))\n",
    "        resolved = set_resolved(G, nodes, length)\n",
    "        for node in resolved:\n",
    "            intensities[node] += +1\n",
    "\n",
    "    small = [key for key, val in intensities.items() if val < tresh]\n",
    "    s = len(small)\n",
    "    nodes_to_add += get_nodes_with_diff_neighbors(G, small)\n",
    "    if s == 0:\n",
    "        break\n",
    "indices = [node_list.index(i) for i in nodes_to_add] \n",
    "node_list_without_indices_to_add = [element for index, element in enumerate(node_list) if index not in indices]\n",
    "c_norm_without_indices_to_add = [element for index, element in enumerate(c_norm) if index not in indices]\n",
    "c_norm_without_indices_to_add = c_norm_without_indices_to_add / sum(c_norm_without_indices_to_add)\n",
    "\n",
    "for _ in tqdm(range(nb_of_iters)):\n",
    "    nodes = nodes_to_add + list(np.random.choice(node_list_without_indices_to_add, p=c_norm_without_indices_to_add, size=len(node_list_without_indices_to_add), replace=False)) # Random set of nodes to test\n",
    "    for nb in range(len(nodes_to_add), len(G)):\n",
    "        if is_resolving_set(G, nodes[:nb], length):\n",
    "            start1_degree_box_real.append(nb)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010363f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1 with betweenness\n",
    "\n",
    "start1_betweenness_box_real = []\n",
    "\n",
    "tresh = 50\n",
    "nb_of_iters = 100\n",
    "\n",
    "G = G_fb\n",
    "nb_of_nodes_in_resolving_set = 20\n",
    "\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "c = nx.betweenness_centrality(G, endpoints=True)\n",
    "\n",
    "# normalize the betweenness centralities\n",
    "c_values = np.array(list(c.values()))\n",
    "c_norm = c_values / sum(c_values)\n",
    "node_list = list(c.keys())\n",
    "ms_b_box_sol_real = []\n",
    "nodes_to_add = []\n",
    "\n",
    "while True:\n",
    "    intensities = {str(key): 0 for key in G.nodes}\n",
    "    for i in range(100):\n",
    "        # Random set of nodes to test\n",
    "        nodes = nodes_to_add + list(np.random.choice(node_list, p=c_norm, size=nb_of_nodes_in_resolving_set, replace=False))\n",
    "        resolved = set_resolved(G, nodes, length)\n",
    "        for node in resolved:\n",
    "            intensities[node] += +1\n",
    "\n",
    "    small = [key for key, val in intensities.items() if val < tresh]\n",
    "    s = len(small)\n",
    "    nodes_to_add += get_nodes_with_diff_neighbors(G, small)\n",
    "    if s == 0:\n",
    "        break\n",
    "indices = [node_list.index(i) for i in nodes_to_add] \n",
    "node_list_without_indices_to_add = [element for index, element in enumerate(node_list) if index not in indices]\n",
    "c_norm_without_indices_to_add = [element for index, element in enumerate(c_norm) if index not in indices]\n",
    "c_norm_without_indices_to_add = c_norm_without_indices_to_add / sum(c_norm_without_indices_to_add)\n",
    "\n",
    "for _ in tqdm(range(nb_of_iters)):\n",
    "\n",
    "    #print(len(node_list_without_indices_to_add))\n",
    "    nodes = nodes_to_add + list(np.random.choice(node_list_without_indices_to_add, p=c_norm_without_indices_to_add, size=len(node_list_without_indices_to_add), replace=False)) # Random set of nodes to test\n",
    "    for nb in range(len(nodes_to_add), len(G)):\n",
    "        if is_resolving_set(G, nodes[:nb], length):\n",
    "            start1_betweenness_box_real.append(nb)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2\n",
    "start2_box_real = []\n",
    "\n",
    "nb_of_iters = 100\n",
    "\n",
    "G = G_fb\n",
    "nb_of_nodes_in_resolving_set = 20\n",
    "\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "for _ in range(nb_of_iters):\n",
    "    nodes_to_add = []\n",
    "\n",
    "    while True:\n",
    "        intensities = {str(key): 0 for key in G.nodes}\n",
    "        for i in range(nb_of_iters):\n",
    "            # Random set of nodes to test\n",
    "            nodes = nodes_to_add + list(np.random.choice(G.nodes, size=nb_of_nodes_in_resolving_set, replace=False))\n",
    "            resolved = set_resolved(G, nodes, length)\n",
    "            for node in resolved:\n",
    "                intensities[node] += 1\n",
    "\n",
    "        hardest_node_to_resolve = min(intensities, key=intensities.get)\n",
    "        nodes_to_add.append(hardest_node_to_resolve)\n",
    "        if is_resolving_set(G, nodes_to_add, length):\n",
    "            start2_box_real.append(len(nodes_to_add))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0148fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_result_real = {'Strategy 1 degree':start1_degree_box_real, \n",
    "                    'Strategy 1 betweenness':start1_betweenness_box_real, \n",
    "                    'Strategy 2':start2_box_real, \n",
    "                    'Random strategy':rand_box_real,\n",
    "                    'Betweenness strategy':betweenness_box_sol,\n",
    "                    'Degree strategy':degree_box_sol\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1481e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for group, values in dict_result_real.items():\n",
    "    fig.add_trace(go.Box(y=values, name=group, boxpoints='all'))\n",
    "\n",
    "    \n",
    "fig.update_layout(\n",
    "    yaxis=dict(\n",
    "        title=\"Size of the resolving set\",\n",
    "        titlefont=dict(size=12, color='black')\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check if the values of the threshold have a big influence\n",
    "\n",
    "dict_tresh = {}\n",
    "\n",
    "tresh_values = [25]\n",
    "nb_graph = 1\n",
    "nb_of_iters = 100\n",
    "\n",
    "G = G_calls\n",
    "print(len(G))\n",
    "nb_of_nodes_in_resolving_set = 20\n",
    "\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "c = nx.degree_centrality(G)\n",
    "\n",
    "# normalize the betweenness centralities\n",
    "c_values = np.array(list(c.values()))\n",
    "c_norm = c_values / sum(c_values)\n",
    "node_list = list(c.keys())\n",
    "for tresh in tresh_values:\n",
    "    ms_b_box_sol_real = []\n",
    "    nodes_to_add = []\n",
    "\n",
    "    while True:\n",
    "        intensities = {str(key): 0 for key in G.nodes}\n",
    "        for i in range(100):\n",
    "            # Random set of nodes to test\n",
    "            nodes = nodes_to_add + list(np.random.choice(node_list, p=c_norm, size=nb_of_nodes_in_resolving_set, replace=False))\n",
    "            resolved = set_resolved(G, nodes, length)\n",
    "            for node in resolved:\n",
    "                intensities[node] += +1\n",
    "\n",
    "        small = [key for key, val in intensities.items() if val < tresh]\n",
    "        s = len(small)\n",
    "        nodes_to_add += get_nodes_with_diff_neighbors(G, small)\n",
    "        if s == 0:\n",
    "            print(\"We manually add {} nodes\".format(len(nodes_to_add)))\n",
    "            break\n",
    "    indices = [node_list.index(i) for i in nodes_to_add] \n",
    "    node_list_without_indices_to_add = [element for index, element in enumerate(node_list) if index not in indices]\n",
    "    c_norm_without_indices_to_add = [element for index, element in enumerate(c_norm) if index not in indices]\n",
    "    c_norm_without_indices_to_add = c_norm_without_indices_to_add / sum(c_norm_without_indices_to_add)\n",
    "    for _ in tqdm(range(nb_of_iters)):\n",
    "        \n",
    "        nodes = nodes_to_add + list(np.random.choice(node_list_without_indices_to_add, p=c_norm_without_indices_to_add, size=len(node_list_without_indices_to_add), replace=False)) # Random set of nodes to test\n",
    "        for nb in range(len(nodes_to_add), len(G)):\n",
    "            if is_resolving_set(G, nodes[:nb], length):\n",
    "                ms_b_box_sol_real.append(nb)\n",
    "                break\n",
    "    dict_tresh[tresh] = ms_b_box_sol_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04821a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for group, values in dict_tresh.items():\n",
    "    fig.add_trace(go.Box(y=values, name=group))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953574e4",
   "metadata": {},
   "source": [
    "## Copenhagen calls graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file and create a graph\n",
    "G_calls = nx.read_edgelist('../Real graphs simulations/Copenhagen graphs/calls.csv/edges.csv', delimiter=',', data=(('timestamp', int),('duration', int)))\n",
    "\n",
    "# print the number of nodes and edges\n",
    "print('Number of nodes:', G_calls.number_of_nodes())\n",
    "print('Number of edges:', G_calls.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f449ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_connected(G_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the small components such that the graph becomes connected\n",
    "connected_components = nx.connected_components(G_calls)\n",
    "biggest = max(connected_components, key=len)\n",
    "G_calls = G_calls.subgraph(biggest)\n",
    "nx.is_connected(G_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf8416",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d0c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betweenness\n",
    "\n",
    "betweenness_box_sol_calls = []\n",
    "\n",
    "nb_of_iters = 100\n",
    "\n",
    "# Generate the random graph and compute shortest paths\n",
    "G = G_calls\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "c = nx.betweenness_centrality(G, endpoints=True)\n",
    "\n",
    "# normalize the centralities\n",
    "c_values = np.array(list(c.values()))\n",
    "c_norm = c_values / sum(c_values)\n",
    "node_list = list(c.keys())\n",
    "\n",
    "for _ in tqdm(range(nb_of_iters)):\n",
    "    nodes = np.random.choice(node_list, p=c_norm, size=len(G), replace=False) # Random set of nodes to test\n",
    "    for nb in range(0, len(G)):\n",
    "        if is_resolving_set(G, nodes[:nb], length):\n",
    "            betweenness_box_sol_calls.append(nb)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d5b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree\n",
    "\n",
    "degree_box_sol_calls = []\n",
    "\n",
    "nb_of_iters = 100\n",
    "\n",
    "# Generate the random graph and compute shortest paths\n",
    "G = G_calls\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "c = nx.degree_centrality(G)\n",
    "\n",
    "# normalize the centralities\n",
    "c_values = np.array(list(c.values()))\n",
    "c_norm = c_values / sum(c_values)\n",
    "node_list = list(c.keys())\n",
    "\n",
    "for _ in range(nb_of_iters):\n",
    "    nodes = np.random.choice(node_list, p=c_norm, size=len(G), replace=False) # Random set of nodes to test\n",
    "    for nb in range(0, len(G)):\n",
    "        if is_resolving_set(G, nodes[:nb], length):\n",
    "            degree_box_sol_calls.append(nb)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random\n",
    "\n",
    "nb_of_iters = 100\n",
    "rand_box_real_calls = []\n",
    "\n",
    "# Generate the random graph and compute shortest paths\n",
    "G = G_calls\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "for _ in tqdm(range(nb_of_iters)):\n",
    "    nodes = list(random.sample(list(G.nodes), len(G)))\n",
    "    for nb in range(0, len(G)):\n",
    "        if is_resolving_set(G, nodes[:nb], length):\n",
    "            rand_box_real_calls.append(nb)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce326bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1 with degree\n",
    "\n",
    "start1_degree_box_real_calls = []\n",
    "\n",
    "tresh = 25\n",
    "nb_of_iters = 100\n",
    "\n",
    "G = G_calls\n",
    "nb_of_nodes_in_resolving_set = 20\n",
    "\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "c = nx.degree_centrality(G)\n",
    "\n",
    "# normalize the betweenness centralities\n",
    "c_values = np.array(list(c.values()))\n",
    "c_norm = c_values / sum(c_values)\n",
    "node_list = list(c.keys())\n",
    "ms_b_box_sol_real = []\n",
    "nodes_to_add = []\n",
    "\n",
    "while True:\n",
    "    intensities = {str(key): 0 for key in G.nodes}\n",
    "    for i in range(100):\n",
    "        # Random set of nodes to test\n",
    "        nodes = nodes_to_add + list(np.random.choice(node_list, p=c_norm, size=nb_of_nodes_in_resolving_set, replace=False))\n",
    "        resolved = set_resolved(G, nodes, length)\n",
    "        for node in resolved:\n",
    "            intensities[node] += +1\n",
    "\n",
    "    small = [key for key, val in intensities.items() if val < tresh]\n",
    "    s = len(small)\n",
    "    nodes_to_add += get_nodes_with_diff_neighbors(G, small)\n",
    "\n",
    "    if s == 0:\n",
    "        print(\"We manually add {} nodes\".format(len(nodes_to_add)))\n",
    "        break\n",
    "indices = [node_list.index(i) for i in nodes_to_add] \n",
    "node_list_without_indices_to_add = [element for index, element in enumerate(node_list) if index not in indices]\n",
    "c_norm_without_indices_to_add = [element for index, element in enumerate(c_norm) if index not in indices]\n",
    "c_norm_without_indices_to_add = c_norm_without_indices_to_add / sum(c_norm_without_indices_to_add)\n",
    "\n",
    "for _ in tqdm(range(nb_of_iters)):\n",
    "    nodes = nodes_to_add + list(np.random.choice(node_list_without_indices_to_add, p=c_norm_without_indices_to_add, size=len(node_list_without_indices_to_add), replace=False)) # Random set of nodes to test\n",
    "    for nb in range(len(nodes_to_add), len(G)):\n",
    "        if is_resolving_set(G, nodes[:nb], length):\n",
    "            start1_degree_box_real_calls.append(nb)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d585a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1 with betweenness\n",
    "\n",
    "start1_betweenness_box_real_calls = []\n",
    "\n",
    "tresh = 15\n",
    "nb_of_iters = 100\n",
    "\n",
    "G = G_calls\n",
    "nb_of_nodes_in_resolving_set = 20\n",
    "\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "c = nx.betweenness_centrality(G, endpoints=True)\n",
    "\n",
    "# normalize the betweenness centralities\n",
    "c_values = np.array(list(c.values()))\n",
    "c_norm = c_values / sum(c_values)\n",
    "node_list = list(c.keys())\n",
    "ms_b_box_sol_real = []\n",
    "nodes_to_add = []\n",
    "\n",
    "while True:\n",
    "    intensities = {str(key): 0 for key in G.nodes}\n",
    "    for i in range(100):\n",
    "        # Random set of nodes to test\n",
    "        nodes = nodes_to_add + list(np.random.choice(node_list, p=c_norm, size=nb_of_nodes_in_resolving_set, replace=False))\n",
    "        resolved = set_resolved(G, nodes, length)\n",
    "        for node in resolved:\n",
    "            intensities[node] += +1\n",
    "\n",
    "    small = [key for key, val in intensities.items() if val < tresh]\n",
    "    s = len(small)\n",
    "    nodes_to_add += get_nodes_with_diff_neighbors(G, small)\n",
    "    if s == 0:\n",
    "        print(\"We manually add {} nodes\".format(len(nodes_to_add)))\n",
    "        break\n",
    "indices = [node_list.index(i) for i in nodes_to_add] \n",
    "node_list_without_indices_to_add = [element for index, element in enumerate(node_list) if index not in indices]\n",
    "c_norm_without_indices_to_add = [element for index, element in enumerate(c_norm) if index not in indices]\n",
    "c_norm_without_indices_to_add = c_norm_without_indices_to_add / sum(c_norm_without_indices_to_add)\n",
    "\n",
    "for _ in tqdm(range(nb_of_iters)):\n",
    "\n",
    "    #print(len(node_list_without_indices_to_add))\n",
    "    nodes = nodes_to_add + list(np.random.choice(node_list_without_indices_to_add, p=c_norm_without_indices_to_add, size=len(node_list_without_indices_to_add), replace=False)) # Random set of nodes to test\n",
    "    for nb in range(len(nodes_to_add), len(G)):\n",
    "        if is_resolving_set(G, nodes[:nb], length):\n",
    "            start1_betweenness_box_real_calls.append(nb)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3214ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2\n",
    "start2_box_real_calls = []\n",
    "\n",
    "nb_of_iters = 100\n",
    "\n",
    "G = G_calls\n",
    "nb_of_nodes_in_resolving_set = 20\n",
    "\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "for _ in range(nb_of_iters):\n",
    "    nodes_to_add = []\n",
    "\n",
    "    while True:\n",
    "        intensities = {str(key): 0 for key in G.nodes}\n",
    "        for i in range(nb_of_iters):\n",
    "            # Random set of nodes to test\n",
    "            nodes = nodes_to_add + list(np.random.choice(G.nodes, size=nb_of_nodes_in_resolving_set, replace=False))\n",
    "            resolved = set_resolved(G, nodes, length)\n",
    "            for node in resolved:\n",
    "                intensities[node] += 1\n",
    "\n",
    "        hardest_node_to_resolve = min(intensities, key=intensities.get)\n",
    "        nodes_to_add.append(hardest_node_to_resolve)\n",
    "        if is_resolving_set(G, nodes_to_add, length):\n",
    "            start2_box_real_calls.append(len(nodes_to_add))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa89da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_result_real = {'Strategy 1 degree':start1_degree_box_real_calls, \n",
    "                    'Strategy 1 betweenness':start1_betweenness_box_real_calls, \n",
    "                    'Strategy 2':start2_box_real_calls, \n",
    "                    'Random strategy':rand_box_real_calls,\n",
    "                    'Betweenness strategy':betweenness_box_sol_calls,\n",
    "                    'Degree strategy':degree_box_sol_calls\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for group, values in dict_result_real.items():\n",
    "    fig.add_trace(go.Box(y=values, name=group, boxpoints='all'))\n",
    "\n",
    "    \n",
    "fig.update_layout(\n",
    "    yaxis=dict(\n",
    "        title=\"Size of the resolving set\",\n",
    "        titlefont=dict(size=12, color='black')\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
